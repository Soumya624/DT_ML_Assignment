{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f5a602f",
   "metadata": {},
   "source": [
    "## Engineer, Generative AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53086a44",
   "metadata": {},
   "source": [
    "### Development of an Adaptable Deep Learning Model for Artistic Style Transfer\n",
    "\n",
    "\n",
    "Design a neural network, preferably a convolutional neural network, with the aim of learning features that capture the distinctive style of an artist. Select a dataset for training and ensure a proper split into training, validation and test sets to evaluate the model effectively. Use suitable loss functions during training to assess both content preservation and style emulation.\n",
    "\n",
    "Following the training phase, develop a method to adapt the learned style features to new artworks while maintaining the original content and artistic integrity. Establish criteria for evaluation, including measures such as stylistic accuracy, content preservation and overall visual appeal, to gauge the effectiveness of the style transfer algorithm.\n",
    "\n",
    "The ultimate objective is to create a versatile system for artistic style transfer that seamlessly incorporates new styles while preserving the essence of the original conten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbba324c",
   "metadata": {},
   "source": [
    "### Design Approach\n",
    "\n",
    "\n",
    "\n",
    "Approaching this task involves first designing a convolutional neural network architecture capable of learning intricate features representing an artist's style. The selected model should be trained on a curated dataset, employing a meticulous split into training, validation and test sets for robust evaluation. During the training phase, the focus lies on incorporating appropriate loss functions to effectively measure content preservation and style emulation.\n",
    "\n",
    "Post-training, the development of a method to adapt learned style features to new artworks while upholding original content and artistic integrity becomes crucial. The approach includes establishing clear evaluation criteria, encompassing factors such as stylistic accuracy, content preservation and overall visual appeal, to systematically assess the performance of the style transfer algorithm.\n",
    "\n",
    "The overarching goal is to create a flexible and effective system for artistic style transfer, capable of seamlessly integrating new styles while maintaining fidelity to the original content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102ccf78",
   "metadata": {},
   "source": [
    "### Prepared By"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcc0c87",
   "metadata": {},
   "source": [
    "Soumya Tarafder | 19AR10036\n",
    "<br/>Indian Institute of Technology, Kharagpur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86270d74",
   "metadata": {},
   "source": [
    "### Code Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d821cba5",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b0c247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55aac68",
   "metadata": {},
   "source": [
    "#### Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301d3c9d",
   "metadata": {},
   "source": [
    "Load and Preprocess the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c6dbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path, max_dimension=400, custom_shape=None):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    if max(image.size) > max_dimension:\n",
    "        new_dimension = max_dimension\n",
    "    else:\n",
    "        new_dimension = max(image.size)\n",
    "\n",
    "    if custom_shape is not None:\n",
    "        new_dimension = custom_shape\n",
    "\n",
    "    transform_pipeline = transforms.Compose([\n",
    "        transforms.Resize(new_dimension),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "    preprocessed_image = transform_pipeline(image)[:3, :, :].unsqueeze(0)\n",
    "\n",
    "    return preprocessed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001c3514",
   "metadata": {},
   "source": [
    "Convert a Tensor to a NumPy Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4990492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_image(tensor):\n",
    "    image = tensor.to(\"cpu\").clone().detach()\n",
    "    image = image.numpy().squeeze().transpose(1, 2, 0)\n",
    "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "    image = np.clip(image, 0, 1)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6988a51",
   "metadata": {},
   "source": [
    "#### Finalize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c84e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frozen_vgg_model():\n",
    "    vgg_model = models.vgg19(pretrained=True).features\n",
    "\n",
    "    for parameter in vgg_model.parameters():\n",
    "        parameter.requires_grad_(False)\n",
    "\n",
    "    return vgg_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657d5ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_return_features(input_image, model, selected_layers=None):\n",
    "    if selected_layers is None:\n",
    "        selected_layers = {'0': 'conv1_1',\n",
    "                           '5': 'conv2_1',\n",
    "                           '10': 'conv3_1',\n",
    "                           '19': 'conv4_1',\n",
    "                           '21': 'conv4_2',\n",
    "                           '28': 'conv5_1'}\n",
    "\n",
    "    features_dict = {}\n",
    "    x = input_image\n",
    "\n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in selected_layers:\n",
    "            features_dict[selected_layers[name]] = x\n",
    "\n",
    "    return features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9e4fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gram_matrix(tensor):\n",
    "    _, d, h, w = tensor.size()\n",
    "    tensor = tensor.view(d, h * w)\n",
    "    gram_matrix = torch.mm(tensor, tensor.t())\n",
    "    return gram_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20ac940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_style_transfer(content_path, style_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    content_image = load_and_preprocess_image(content_path).to(device)\n",
    "    style_image = load_and_preprocess_image(style_path, custom_shape=content_image.shape[-2:]).to(device)\n",
    "\n",
    "    vgg_model = get_frozen_vgg_model().to(device)\n",
    "\n",
    "    content_features = extract_and_return_features(content_image, vgg_model)\n",
    "    style_features = extract_and_return_features(style_image, vgg_model)\n",
    "\n",
    "    style_grams = {layer: calculate_gram_matrix(style_features[layer]) for layer in style_features}\n",
    "\n",
    "    generated_image = content_image.clone().requires_grad_(True).to(device)\n",
    "\n",
    "    style_weights = {'conv1_1': 1.0,\n",
    "                     'conv2_1': 0.75,\n",
    "                     'conv3_1': 0.2,\n",
    "                     'conv4_1': 0.2,\n",
    "                     'conv5_1': 0.2}\n",
    "\n",
    "    content_weight = 1\n",
    "    style_weight = 1e9\n",
    "\n",
    "    optimizer = optim.Adam([generated_image], lr=0.003)\n",
    "    num_steps = 500\n",
    "\n",
    "    for step in range(1, num_steps + 1):\n",
    "        generated_features = extract_and_return_features(generated_image, vgg_model)\n",
    "\n",
    "        content_loss = torch.mean((generated_features['conv4_2'] - content_features['conv4_2'])**2)\n",
    "\n",
    "        style_loss = 0\n",
    "        for layer in style_weights:\n",
    "            target_feature = generated_features[layer]\n",
    "            target_gram = calculate_gram_matrix(target_feature)\n",
    "            _, d, h, w = target_feature.shape\n",
    "            style_gram = style_grams[layer]\n",
    "            layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n",
    "            style_loss += layer_style_loss / (d * h * w)\n",
    "\n",
    "        total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return generated_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2d0865",
   "metadata": {},
   "source": [
    "#### Final Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfe37c1",
   "metadata": {},
   "source": [
    "Take Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f31c5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image_path = 'content.jpg'\n",
    "style_image_path = 'style.jpg'\n",
    "stylized_image = perform_style_transfer(content_image_path, style_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36364214",
   "metadata": {},
   "source": [
    "Display Content, Style and Stylized Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134976ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 15))\n",
    "ax1.imshow(tensor_to_image(load_and_preprocess_image(content_image_path).to(\"cpu\")))\n",
    "ax1.set_title(\"Content Image\", fontsize=20)\n",
    "ax2.imshow(tensor_to_image(load_and_preprocess_image(style_image_path).to(\"cpu\")))\n",
    "ax2.set_title(\"Style Image\", fontsize=20)\n",
    "ax3.imshow(tensor_to_image(stylized_image))\n",
    "ax3.set_title(\"Stylized Image\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698421ba",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e17e7ed",
   "metadata": {},
   "source": [
    "In conclusion, the provided code employs deep neural style transfer to seamlessly blend the content of one image with the artistic style of another. Leveraging the power of the VGG19 convolutional neural network and PyTorch, the script transforms input images into visually captivating compositions.\n",
    "\n",
    "The modular design, with descriptive function and variable names, enhances code readability and maintainability. By utilizing gram matrices to capture style features and optimizing a generated image with a combination of content and style losses, the algorithm iteratively refines the output over a specified number of steps.\n",
    "\n",
    "The final stylized image, showcased through Matplotlib, demonstrates the successful fusion of content and style, highlighting the effectiveness of the implemented style transfer technique."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
